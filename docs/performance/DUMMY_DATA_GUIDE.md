# 📊 더미 데이터 시스템 가이드

## 🎯 개요

성능 테스트의 정확성과 일관성을 위해 대용량 더미 데이터를 CSV 파일로 관리하는 시스템입니다.

## 🚀 주요 특징

### 1. 자동 CSV 관리
- **파일 존재 확인**: 기존 CSV 파일이 있으면 로드
- **자동 생성**: 파일이 없으면 새로 생성
- **캐싱**: 한 번 생성된 데이터는 재사용

### 2. 현실적인 데이터 분포
- **지역 기반**: 서울 주요 7개 구 중심
- **가중치 적용**: 강남구 25%, 마포구 20% 등 실제 밀도 반영
- **클러스터링**: 70%는 중심 근처, 30%는 넓은 범위 분산

### 3. 다양한 비즈니스 데이터
- **8개 카테고리**: restaurant, cafe, retail, office, hospital, school, park, bank
- **현실적 가치**: 카테고리별 기본 가치 + 지역 가중치
- **추가 속성**: rating, review_count, business_type, is_active

## 📋 데이터 구조

### CSV 파일 컬럼
```
id, name, lat, lng, category, business_type, value, area, 
rating, review_count, created_at, is_active
```

### 예시 데이터
```csv
1,Restaurant_000001,37.520156,127.053421,restaurant,franchise,75234.12,강남구,4.2,156,2024-08-15T10:30:45,True
2,Cafe_000002,37.558234,126.927891,cafe,local,42156.78,마포구,3.8,89,2024-09-02T14:20:15,True
```

## 🔧 사용 방법

### 1. 기본 실행
```python
# 기본 150,000개 데이터
performance_test = HexagonVsSquarePerformance()
```

### 2. 커스텀 크기
```python
# 500,000개 데이터
performance_test = HexagonVsSquarePerformance(500000)
```

### 3. 데이터 준비
```python
# 자동으로 CSV 파일 확인 후 로드/생성
data = performance_test.load_or_create_dummy_data()
```

## 📊 성능 최적화

### 1. 파일 이름 규칙
```
dummy_data_{크기}.csv
예: dummy_data_150000.csv, dummy_data_500000.csv
```

### 2. 캐싱 전략
- 동일한 크기의 데이터는 재생성하지 않음
- 파일 로딩이 생성보다 약 10배 빠름
- 메모리 효율적인 pandas 사용

### 3. 진행상황 표시
```
데이터 생성 중...
  진행: 10,000/150,000 (6.7%) - 2.3초
  진행: 20,000/150,000 (13.3%) - 4.8초
  ...
```

## 🎲 데이터 분포 특성

### 지역별 가중치
| 지역 | 비율 | 특징 |
|------|------|------|
| 강남구 | 25% | 높은 가치 (1.5배 가중치) |
| 마포구 | 20% | 카페/레스토랑 밀집 |
| 중구 | 15% | 금융/오피스 중심 |
| 서초구 | 15% | 강남과 유사한 고가치 |
| 송파구 | 10% | 쇼핑/엔터테인먼트 |
| 영등포구 | 10% | 오피스 밀집 |
| 용산구 | 5% | 혼합 상업지역 |

### 카테고리별 기본 가치
| 카테고리 | 기본 가치 | 특징 |
|----------|-----------|------|
| office | 200,000 | 가장 높은 가치 |
| hospital | 150,000 | 의료 서비스 |
| bank | 120,000 | 금융 서비스 |
| school | 100,000 | 교육 시설 |
| retail | 80,000 | 소매업 |
| restaurant | 50,000 | 음식점 |
| cafe | 30,000 | 카페 |
| park | 10,000 | 공공시설 |

## 💡 활용 팁

### 1. 테스트 데이터 샘플링
```python
# 활성 데이터만 50,000개 샘플링
sample = performance_test.prepare_performance_data(50000)
```

### 2. 파일 크기 예상
- 150,000개: 약 20MB
- 500,000개: 약 65MB
- 1,000,000개: 약 130MB

### 3. 생성 시간 예상
- 150,000개: 약 30-45초
- 500,000개: 약 2-3분
- 1,000,000개: 약 5-7분

## ⚠️ 주의사항

### 1. 디스크 공간
대용량 CSV 파일로 인한 디스크 공간 사용에 주의하세요.

### 2. 메모리 사용량
전체 데이터를 메모리에 로드하므로 시스템 메모리를 고려하세요.

### 3. 재현성
동일한 `random_seed(42)`를 사용하여 재현 가능한 결과를 보장합니다.

## 🔍 트러블슈팅

### CSV 파일 손상 시
```python
# 파일 삭제 후 재실행
import os
os.remove('/path/to/dummy_data_150000.csv')
```

### 메모리 부족 시
```python
# 더 작은 크기로 테스트
performance_test = HexagonVsSquarePerformance(50000)
```

### 속도 개선
```python
# SSD 사용, 충분한 RAM 확보
# pandas.read_csv() 옵션 최적화 고려
```

---

이 시스템으로 일관되고 현실적인 대용량 데이터를 통한 정확한 성능 비교가 가능합니다.